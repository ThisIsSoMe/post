Set max num of threads to 4
Preprocess the data
Corpus(
  num of sentences: 16091
  num of words: 54303
  num of tags: 32
  num of chars: 7477
)

Load the dataset
  size of trainset: 16091
  size of devset: 803
  size of testset: 1910

Create Neural Network
  vocdim: 54303
  embdim: 100
  outdim: 32

ATTN(
  (embed): Embedding(54303, 100)
  (encoder): Encoder(
    (layers): ModuleList(
      (0): Layer(
        (attn): MultiHeadAttn(
          (softmax): Softmax()
          (proj): Linear(in_features=100, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
        (ffn): PosWiseFFN(
          (w1): Sequential(
            (0): Linear(in_features=100, out_features=200, bias=True)
            (1): ReLU()
          )
          (w2): Linear(in_features=200, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
      )
      (1): Layer(
        (attn): MultiHeadAttn(
          (softmax): Softmax()
          (proj): Linear(in_features=100, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
        (ffn): PosWiseFFN(
          (w1): Sequential(
            (0): Linear(in_features=100, out_features=200, bias=True)
            (1): ReLU()
          )
          (w2): Linear(in_features=200, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
      )
      (2): Layer(
        (attn): MultiHeadAttn(
          (softmax): Softmax()
          (proj): Linear(in_features=100, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
        (ffn): PosWiseFFN(
          (w1): Sequential(
            (0): Linear(in_features=100, out_features=200, bias=True)
            (1): ReLU()
          )
          (w2): Linear(in_features=200, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
      )
      (3): Layer(
        (attn): MultiHeadAttn(
          (softmax): Softmax()
          (proj): Linear(in_features=100, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
        (ffn): PosWiseFFN(
          (w1): Sequential(
            (0): Linear(in_features=100, out_features=200, bias=True)
            (1): ReLU()
          )
          (w2): Linear(in_features=200, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
      )
      (4): Layer(
        (attn): MultiHeadAttn(
          (softmax): Softmax()
          (proj): Linear(in_features=100, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
        (ffn): PosWiseFFN(
          (w1): Sequential(
            (0): Linear(in_features=100, out_features=200, bias=True)
            (1): ReLU()
          )
          (w2): Linear(in_features=200, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
      )
      (5): Layer(
        (attn): MultiHeadAttn(
          (softmax): Softmax()
          (proj): Linear(in_features=100, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
        (ffn): PosWiseFFN(
          (w1): Sequential(
            (0): Linear(in_features=100, out_features=200, bias=True)
            (1): ReLU()
          )
          (w2): Linear(in_features=200, out_features=100, bias=True)
          (norm): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)
          (drop): Dropout(p=0.1)
        )
      )
    )
    (drop): Dropout(p=0.1)
  )
  (out): Linear(in_features=100, out_features=32, bias=True)
  (crf): CRF()
  (lossfn): CrossEntropyLoss()
)

Use Adam optimizer to train the network
  epochs: 100
  batch_size: 25
  interval: 10
  eta: 0.001

Epoch: 1 / 100:
train: Loss: 7.0164 Accuracy: 398628 / 437991 = 91.01%
dev:   Loss: 6.8313 Accuracy: 18403 / 20454 = 89.97%
0:04:23.951096s elapsed

Epoch: 2 / 100:
train: Loss: 4.9270 Accuracy: 409834 / 437991 = 93.57%
dev:   Loss: 5.6637 Accuracy: 18754 / 20454 = 91.69%
0:04:32.161276s elapsed

Epoch: 3 / 100:
train: Loss: 3.9443 Accuracy: 414178 / 437991 = 94.56%
dev:   Loss: 5.3616 Accuracy: 18852 / 20454 = 92.17%
0:04:12.115757s elapsed

Epoch: 4 / 100:
train: Loss: 3.3542 Accuracy: 416863 / 437991 = 95.18%
dev:   Loss: 5.0499 Accuracy: 18892 / 20454 = 92.36%
0:04:27.421933s elapsed

Epoch: 5 / 100:
train: Loss: 2.8553 Accuracy: 420576 / 437991 = 96.02%
dev:   Loss: 4.8828 Accuracy: 19013 / 20454 = 92.95%
0:04:07.230325s elapsed

Epoch: 6 / 100:
train: Loss: 2.4672 Accuracy: 422849 / 437991 = 96.54%
dev:   Loss: 5.0004 Accuracy: 19041 / 20454 = 93.09%
0:04:29.641681s elapsed

Epoch: 7 / 100:
train: Loss: 2.1361 Accuracy: 425521 / 437991 = 97.15%
dev:   Loss: 4.8024 Accuracy: 19105 / 20454 = 93.40%
0:05:00.650926s elapsed

Epoch: 8 / 100:
train: Loss: 1.8835 Accuracy: 426727 / 437991 = 97.43%
dev:   Loss: 5.0947 Accuracy: 19110 / 20454 = 93.43%
0:04:59.894511s elapsed

Epoch: 9 / 100:
train: Loss: 1.6489 Accuracy: 428059 / 437991 = 97.73%
dev:   Loss: 5.0209 Accuracy: 19063 / 20454 = 93.20%
0:04:53.544931s elapsed

Epoch: 10 / 100:
train: Loss: 1.3278 Accuracy: 430598 / 437991 = 98.31%
dev:   Loss: 4.9794 Accuracy: 19120 / 20454 = 93.48%
0:05:00.759531s elapsed

Epoch: 11 / 100:
train: Loss: 1.1870 Accuracy: 431366 / 437991 = 98.49%
dev:   Loss: 5.4461 Accuracy: 19088 / 20454 = 93.32%
0:04:51.828593s elapsed

Epoch: 12 / 100:
train: Loss: 0.9889 Accuracy: 432813 / 437991 = 98.82%
dev:   Loss: 5.4884 Accuracy: 19103 / 20454 = 93.39%
0:04:50.394705s elapsed

Epoch: 13 / 100:
train: Loss: 0.8945 Accuracy: 433197 / 437991 = 98.91%
dev:   Loss: 5.5576 Accuracy: 19123 / 20454 = 93.49%
0:05:02.880356s elapsed

Epoch: 14 / 100:
train: Loss: 0.7802 Accuracy: 433897 / 437991 = 99.07%
dev:   Loss: 5.4520 Accuracy: 19099 / 20454 = 93.38%
0:04:52.270240s elapsed

Epoch: 15 / 100:
train: Loss: 0.7158 Accuracy: 434346 / 437991 = 99.17%
dev:   Loss: 5.6558 Accuracy: 19075 / 20454 = 93.26%
0:04:45.874940s elapsed

Epoch: 16 / 100:
train: Loss: 0.6113 Accuracy: 434993 / 437991 = 99.32%
dev:   Loss: 5.5829 Accuracy: 19081 / 20454 = 93.29%
0:04:52.171606s elapsed

Epoch: 17 / 100:
train: Loss: 0.5599 Accuracy: 435188 / 437991 = 99.36%
dev:   Loss: 6.0131 Accuracy: 19088 / 20454 = 93.32%
0:04:51.552818s elapsed

Epoch: 18 / 100:
train: Loss: 0.4776 Accuracy: 435577 / 437991 = 99.45%
dev:   Loss: 6.3658 Accuracy: 19058 / 20454 = 93.17%
0:04:51.549420s elapsed

Epoch: 19 / 100:
train: Loss: 0.4393 Accuracy: 435960 / 437991 = 99.54%
dev:   Loss: 6.0923 Accuracy: 19028 / 20454 = 93.03%
0:04:53.080006s elapsed

Epoch: 20 / 100:
train: Loss: 0.3498 Accuracy: 436309 / 437991 = 99.62%
dev:   Loss: 6.4678 Accuracy: 19096 / 20454 = 93.36%
0:04:49.985035s elapsed

Epoch: 21 / 100:
train: Loss: 0.3189 Accuracy: 436487 / 437991 = 99.66%
dev:   Loss: 6.6194 Accuracy: 19063 / 20454 = 93.20%
0:04:52.547090s elapsed

Epoch: 22 / 100:
train: Loss: 0.2781 Accuracy: 436778 / 437991 = 99.72%
dev:   Loss: 6.5618 Accuracy: 19056 / 20454 = 93.17%
0:04:53.781249s elapsed

Epoch: 23 / 100:
train: Loss: 0.2898 Accuracy: 436684 / 437991 = 99.70%
dev:   Loss: 6.7134 Accuracy: 19049 / 20454 = 93.13%
0:04:53.573515s elapsed

Epoch: 24 / 100:
train: Loss: 0.2448 Accuracy: 436842 / 437991 = 99.74%
dev:   Loss: 6.7212 Accuracy: 19091 / 20454 = 93.34%
0:05:50.857596s elapsed

max accuracy of dev is 93.49% at epoch 13
mean time of each epoch is 0:04:48.321631s

test:  Loss: 6.2107 Accuracy: 46813 / 50319 = 93.03%
1:55:25.308058s elapsed

