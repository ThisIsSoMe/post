nohup: 忽略输入
Set max num of threads to 4
Preprocess the data
Corpus(
  num of sentences: 16091
  num of words: 383647
  num of tags: 32
  num of chars: 7477
)

Load the dataset
  size of trainset: 16091
  size of devset: 803
  size of testset: 1910

Create Neural Network
  window: 1
  vocdim: 383647
  chrdim: 7477
  embdim: 100
  char_embdim: 100
  hiddim: 300
  outdim: 32
  lossfn: cross_entropy

LSTM(
  (embed): Embedding(383647, 100)
  (clstm): CharLSTM(
    (embed): Embedding(7477, 100)
    (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)
  )
  (wlstm): LSTM(300, 150, batch_first=True, bidirectional=True)
  (attn): Attention(
    (attention): ScaledDotProductAttention(
      (dropout): Dropout(p=0.1)
    )
    (layer_norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=900, out_features=300, bias=True)
    (dropout): Dropout(p=0.1)
  )
  (out): Linear(in_features=300, out_features=32, bias=True)
  (crf): CRF()
  (dropout): Dropout(p=0.6)
)

Use Adam optimizer to train the network
  epochs: 100
  batch_size: 25
  interval: 10
  eta: 0.001

Epoch: 1 / 100:
train: Loss: 6.6156 Accuracy: 400921 / 437991 = 91.54%
dev:   Loss: 6.2755 Accuracy: 18656 / 20454 = 91.21%
0:06:28.625246s elapsed

Epoch: 2 / 100:
train: Loss: 4.8672 Accuracy: 410069 / 437991 = 93.62%
dev:   Loss: 5.1372 Accuracy: 18931 / 20454 = 92.55%
0:06:42.180021s elapsed

Epoch: 3 / 100:
train: Loss: 3.9444 Accuracy: 415357 / 437991 = 94.83%
dev:   Loss: 4.5526 Accuracy: 19093 / 20454 = 93.35%
0:06:49.787153s elapsed

Epoch: 4 / 100:
train: Loss: 3.4031 Accuracy: 418471 / 437991 = 95.54%
dev:   Loss: 4.3362 Accuracy: 19206 / 20454 = 93.90%
0:08:02.063652s elapsed

Epoch: 5 / 100:
train: Loss: 3.0387 Accuracy: 420257 / 437991 = 95.95%
dev:   Loss: 4.1504 Accuracy: 19256 / 20454 = 94.14%
0:08:11.856266s elapsed

Epoch: 6 / 100:
train: Loss: 2.7336 Accuracy: 421975 / 437991 = 96.34%
dev:   Loss: 3.9724 Accuracy: 19367 / 20454 = 94.69%
0:07:30.506755s elapsed

Epoch: 7 / 100:
train: Loss: 2.4249 Accuracy: 424113 / 437991 = 96.83%
dev:   Loss: 3.9197 Accuracy: 19383 / 20454 = 94.76%
0:06:45.026709s elapsed

Epoch: 8 / 100:
train: Loss: 2.2298 Accuracy: 425229 / 437991 = 97.09%
dev:   Loss: 3.9409 Accuracy: 19401 / 20454 = 94.85%
0:06:48.562380s elapsed

Epoch: 9 / 100:
train: Loss: 2.0285 Accuracy: 426422 / 437991 = 97.36%
dev:   Loss: 3.8115 Accuracy: 19441 / 20454 = 95.05%
0:06:35.121996s elapsed

Epoch: 10 / 100:
train: Loss: 1.9403 Accuracy: 426932 / 437991 = 97.48%
dev:   Loss: 3.7063 Accuracy: 19435 / 20454 = 95.02%
0:06:35.097174s elapsed

Epoch: 11 / 100:
train: Loss: 1.7743 Accuracy: 427872 / 437991 = 97.69%
dev:   Loss: 3.8594 Accuracy: 19485 / 20454 = 95.26%
0:06:33.998279s elapsed

Epoch: 12 / 100:
train: Loss: 1.6368 Accuracy: 428558 / 437991 = 97.85%
dev:   Loss: 3.7483 Accuracy: 19523 / 20454 = 95.45%
0:06:33.553042s elapsed

Epoch: 13 / 100:
train: Loss: 1.5413 Accuracy: 429356 / 437991 = 98.03%
dev:   Loss: 3.8532 Accuracy: 19484 / 20454 = 95.26%
0:06:35.339229s elapsed

Epoch: 14 / 100:
train: Loss: 1.4201 Accuracy: 430142 / 437991 = 98.21%
dev:   Loss: 3.9213 Accuracy: 19473 / 20454 = 95.20%
0:07:10.579424s elapsed

Epoch: 15 / 100:
train: Loss: 1.3791 Accuracy: 430249 / 437991 = 98.23%
dev:   Loss: 3.7506 Accuracy: 19485 / 20454 = 95.26%
0:07:09.395215s elapsed

Epoch: 16 / 100:
train: Loss: 1.2376 Accuracy: 431097 / 437991 = 98.43%
dev:   Loss: 4.0730 Accuracy: 19509 / 20454 = 95.38%
0:07:07.792372s elapsed

Epoch: 17 / 100:
train: Loss: 1.1659 Accuracy: 431549 / 437991 = 98.53%
dev:   Loss: 4.1070 Accuracy: 19511 / 20454 = 95.39%
0:06:45.183743s elapsed

Epoch: 18 / 100:
train: Loss: 1.0988 Accuracy: 431854 / 437991 = 98.60%
dev:   Loss: 4.0340 Accuracy: 19511 / 20454 = 95.39%
0:06:40.223969s elapsed

Epoch: 19 / 100:
train: Loss: 1.0323 Accuracy: 432206 / 437991 = 98.68%
dev:   Loss: 3.9915 Accuracy: 19499 / 20454 = 95.33%
0:06:46.218076s elapsed

Epoch: 20 / 100:
train: Loss: 0.9637 Accuracy: 432739 / 437991 = 98.80%
dev:   Loss: 3.9327 Accuracy: 19538 / 20454 = 95.52%
0:07:02.711123s elapsed

Epoch: 21 / 100:
train: Loss: 0.9111 Accuracy: 433031 / 437991 = 98.87%
dev:   Loss: 4.0349 Accuracy: 19541 / 20454 = 95.54%
0:07:12.860158s elapsed

Epoch: 22 / 100:
train: Loss: 0.8823 Accuracy: 433092 / 437991 = 98.88%
dev:   Loss: 4.1093 Accuracy: 19519 / 20454 = 95.43%
0:07:00.548523s elapsed

Epoch: 23 / 100:
train: Loss: 0.8227 Accuracy: 433453 / 437991 = 98.96%
dev:   Loss: 4.2169 Accuracy: 19524 / 20454 = 95.45%
0:06:40.084941s elapsed

Epoch: 24 / 100:
train: Loss: 0.7933 Accuracy: 433683 / 437991 = 99.02%
dev:   Loss: 4.3082 Accuracy: 19489 / 20454 = 95.28%
0:06:37.957903s elapsed

Epoch: 25 / 100:
train: Loss: 0.7420 Accuracy: 433940 / 437991 = 99.08%
dev:   Loss: 4.3850 Accuracy: 19512 / 20454 = 95.39%
0:06:44.169014s elapsed

Epoch: 26 / 100:
train: Loss: 0.6989 Accuracy: 434126 / 437991 = 99.12%
dev:   Loss: 4.4275 Accuracy: 19505 / 20454 = 95.36%
0:06:56.573800s elapsed

Epoch: 27 / 100:
train: Loss: 0.6765 Accuracy: 434264 / 437991 = 99.15%
dev:   Loss: 4.1883 Accuracy: 19514 / 20454 = 95.40%
0:06:41.504793s elapsed

Epoch: 28 / 100:
train: Loss: 0.6290 Accuracy: 434593 / 437991 = 99.22%
dev:   Loss: 4.5052 Accuracy: 19479 / 20454 = 95.23%
0:06:35.474043s elapsed

Epoch: 29 / 100:
train: Loss: 0.6435 Accuracy: 434305 / 437991 = 99.16%
dev:   Loss: 4.6799 Accuracy: 19504 / 20454 = 95.36%
0:06:43.344671s elapsed

Epoch: 30 / 100:
train: Loss: 0.5660 Accuracy: 434870 / 437991 = 99.29%
dev:   Loss: 4.5512 Accuracy: 19520 / 20454 = 95.43%
0:06:43.517917s elapsed

Epoch: 31 / 100:
train: Loss: 0.5902 Accuracy: 434683 / 437991 = 99.24%
dev:   Loss: 4.5765 Accuracy: 19502 / 20454 = 95.35%
0:06:34.663860s elapsed

Epoch: 32 / 100:
train: Loss: 0.5272 Accuracy: 435131 / 437991 = 99.35%
dev:   Loss: 4.3384 Accuracy: 19519 / 20454 = 95.43%
0:06:44.517969s elapsed

max accuracy of dev is 95.54% at epoch 21
mean time of each epoch is 0:06:52.782482s

test:  Loss: 3.8926 Accuracy: 47986 / 50319 = 95.36%
3:40:19.626939s elapsed

