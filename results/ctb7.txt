nohup: 忽略输入
Set max num of threads to 4
Preprocess the data
Corpus(
  num of sentences: 46572
  num of words: 394637
  num of tags: 35
  num of chars: 7511
)

Load the dataset
  size of trainset: 46572
  size of devset: 2079
  size of testset: 2796

Create Neural Network
  window: 1
  vocdim: 394637
  chrdim: 7511
  embdim: 100
  char_embdim: 100
  hiddim: 300
  outdim: 35
  lossfn: cross_entropy

LSTM_CHAR(
  (embed): Embedding(394637, 100)
  (clstm): CharLSTM(
    (embed): Embedding(7511, 100)
    (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)
  )
  (wlstm): LSTM(300, 150, batch_first=True, bidirectional=True)
  (attn): ATTN(
    (attn): MultiHeadAttention(
      (attn): ScaledDotProductAttention(
        (dropout): Dropout(p=0.1)
      )
      (norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=900, out_features=300, bias=True)
      (dropout): Dropout(p=0.1)
    )
    (ffn): PosWiseFFN(
      (w1): Linear(in_features=300, out_features=300, bias=True)
      (w2): Linear(in_features=300, out_features=300, bias=True)
      (norm): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
  )
  (out): Linear(in_features=300, out_features=35, bias=True)
  (crf): CRF()
  (dropout): Dropout(p=0.6)
)

Use Adam optimizer to train the network
  epochs: 100
  batch_size: 25
  interval: 10
  eta: 0.001

Epoch: 1 / 100:
train: Loss: 4.0827 Accuracy: 990390 / 1057943 = 93.61%
dev:   Loss: 5.4844 Accuracy: 55748 / 59955 = 92.98%
0:22:05.788147s elapsed

Epoch: 2 / 100:
train: Loss: 3.1006 Accuracy: 1005278 / 1057943 = 95.02%
dev:   Loss: 4.5239 Accuracy: 56560 / 59955 = 94.34%
0:22:44.671106s elapsed

Epoch: 3 / 100:
train: Loss: 2.6367 Accuracy: 1012343 / 1057943 = 95.69%
dev:   Loss: 4.1525 Accuracy: 56788 / 59955 = 94.72%
0:22:58.702456s elapsed

Epoch: 4 / 100:
train: Loss: 2.3558 Accuracy: 1016941 / 1057943 = 96.12%
dev:   Loss: 4.0489 Accuracy: 56964 / 59955 = 95.01%
0:23:18.397774s elapsed

Epoch: 5 / 100:
train: Loss: 2.1300 Accuracy: 1020678 / 1057943 = 96.48%
dev:   Loss: 3.8003 Accuracy: 57101 / 59955 = 95.24%
0:23:12.359865s elapsed

Epoch: 6 / 100:
train: Loss: 1.9575 Accuracy: 1024143 / 1057943 = 96.81%
dev:   Loss: 3.8354 Accuracy: 57129 / 59955 = 95.29%
0:23:17.064316s elapsed

Epoch: 7 / 100:
train: Loss: 1.8595 Accuracy: 1025421 / 1057943 = 96.93%
dev:   Loss: 3.7680 Accuracy: 57218 / 59955 = 95.43%
0:23:16.580552s elapsed

Epoch: 8 / 100:
train: Loss: 1.7387 Accuracy: 1027599 / 1057943 = 97.13%
dev:   Loss: 3.7308 Accuracy: 57226 / 59955 = 95.45%
0:23:25.811507s elapsed

Epoch: 9 / 100:
train: Loss: 1.6431 Accuracy: 1029554 / 1057943 = 97.32%
dev:   Loss: 3.7289 Accuracy: 57315 / 59955 = 95.60%
0:23:26.490486s elapsed

Epoch: 10 / 100:
train: Loss: 1.5495 Accuracy: 1030738 / 1057943 = 97.43%
dev:   Loss: 3.7023 Accuracy: 57349 / 59955 = 95.65%
0:23:23.167025s elapsed

/data/zhangyu/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type MultiHeadAttention. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
Epoch: 11 / 100:
train: Loss: 1.4418 Accuracy: 1033119 / 1057943 = 97.65%
dev:   Loss: 3.7028 Accuracy: 57354 / 59955 = 95.66%
0:23:18.188232s elapsed

Epoch: 12 / 100:
train: Loss: 1.3655 Accuracy: 1034595 / 1057943 = 97.79%
dev:   Loss: 3.6965 Accuracy: 57337 / 59955 = 95.63%
0:22:53.292595s elapsed

Epoch: 13 / 100:
train: Loss: 1.3190 Accuracy: 1035422 / 1057943 = 97.87%
dev:   Loss: 3.7128 Accuracy: 57386 / 59955 = 95.72%
0:21:59.697845s elapsed

Epoch: 14 / 100:
train: Loss: 1.2527 Accuracy: 1036646 / 1057943 = 97.99%
dev:   Loss: 3.8189 Accuracy: 57375 / 59955 = 95.70%
0:22:06.942601s elapsed

Epoch: 15 / 100:
train: Loss: 1.2224 Accuracy: 1037137 / 1057943 = 98.03%
dev:   Loss: 3.8270 Accuracy: 57431 / 59955 = 95.79%
0:22:03.344513s elapsed

Epoch: 16 / 100:
train: Loss: 1.1548 Accuracy: 1038285 / 1057943 = 98.14%
dev:   Loss: 3.8133 Accuracy: 57368 / 59955 = 95.69%
0:22:08.084383s elapsed

Epoch: 17 / 100:
train: Loss: 1.1258 Accuracy: 1038895 / 1057943 = 98.20%
dev:   Loss: 3.8179 Accuracy: 57347 / 59955 = 95.65%
0:22:06.661129s elapsed

Epoch: 18 / 100:
train: Loss: 1.0892 Accuracy: 1039509 / 1057943 = 98.26%
dev:   Loss: 3.8653 Accuracy: 57402 / 59955 = 95.74%
0:21:55.469173s elapsed

Epoch: 19 / 100:
train: Loss: 1.0550 Accuracy: 1040002 / 1057943 = 98.30%
dev:   Loss: 3.9943 Accuracy: 57376 / 59955 = 95.70%
0:21:52.872915s elapsed

Epoch: 20 / 100:
train: Loss: 0.9878 Accuracy: 1041698 / 1057943 = 98.46%
dev:   Loss: 3.8851 Accuracy: 57378 / 59955 = 95.70%
0:21:36.201604s elapsed

Epoch: 21 / 100:
train: Loss: 0.9425 Accuracy: 1042236 / 1057943 = 98.52%
dev:   Loss: 3.9820 Accuracy: 57406 / 59955 = 95.75%
0:20:50.725637s elapsed

Epoch: 22 / 100:
train: Loss: 0.9444 Accuracy: 1042155 / 1057943 = 98.51%
dev:   Loss: 4.0483 Accuracy: 57378 / 59955 = 95.70%
0:22:25.938923s elapsed

Epoch: 23 / 100:
train: Loss: 0.9022 Accuracy: 1043125 / 1057943 = 98.60%
dev:   Loss: 3.9395 Accuracy: 57412 / 59955 = 95.76%
0:21:51.931750s elapsed

Epoch: 24 / 100:
train: Loss: 0.8962 Accuracy: 1043204 / 1057943 = 98.61%
dev:   Loss: 4.0930 Accuracy: 57348 / 59955 = 95.65%
0:21:57.437417s elapsed

Epoch: 25 / 100:
train: Loss: 0.8505 Accuracy: 1044148 / 1057943 = 98.70%
dev:   Loss: 4.0948 Accuracy: 57346 / 59955 = 95.65%
0:22:03.626817s elapsed

Epoch: 26 / 100:
train: Loss: 0.8327 Accuracy: 1044291 / 1057943 = 98.71%
dev:   Loss: 4.1157 Accuracy: 57377 / 59955 = 95.70%
0:22:01.193427s elapsed

max accuracy of dev is 95.79% at epoch 15
mean time of each epoch is 0:22:28.486238s

test:  Loss: 4.0179 Accuracy: 77947 / 81578 = 95.55%
9:44:35.222073s elapsed

