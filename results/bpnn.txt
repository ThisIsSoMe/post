nohup: 忽略输入
Set max num of threads to 4
Preprocess the data
Corpus(
  num of sentences: 16091
  num of words: 383647
  num of tags: 32
  num of chars: 7477
)

Load the dataset
  size of trainset: 16091
  size of devset: 803
  size of testset: 1910

Create Neural Network
  window: 5
  vocdim: 383647
  embdim: 100
  hiddim: 300
  outdim: 32
  lossfn: cross_entropy

BPNN(
  (embed): Embedding(383647, 100)
  (hid): Linear(in_features=500, out_features=300, bias=True)
  (out): Linear(in_features=300, out_features=32, bias=True)
  (dropout): Dropout(p=0.5)
)

Use Adam optimizer to train the network
  epochs: 100
  batch_size: 25
  interval: 10
  eta: 0.001

Epoch: 1 / 100:
train: Loss: 0.2417 Accuracy: 403644 / 437991 = 92.16%
dev:   Loss: 0.2737 Accuracy: 18607 / 20454 = 90.97%
0:02:26.938594s elapsed

Epoch: 2 / 100:
train: Loss: 0.1605 Accuracy: 414377 / 437991 = 94.61%
dev:   Loss: 0.2371 Accuracy: 18857 / 20454 = 92.19%
0:02:26.835914s elapsed

Epoch: 3 / 100:
train: Loss: 0.1201 Accuracy: 419962 / 437991 = 95.88%
dev:   Loss: 0.2299 Accuracy: 18972 / 20454 = 92.75%
0:02:28.505691s elapsed

Epoch: 4 / 100:
train: Loss: 0.0930 Accuracy: 423994 / 437991 = 96.80%
dev:   Loss: 0.2194 Accuracy: 19057 / 20454 = 93.17%
0:02:30.990130s elapsed

Epoch: 5 / 100:
train: Loss: 0.0735 Accuracy: 426748 / 437991 = 97.43%
dev:   Loss: 0.2188 Accuracy: 19095 / 20454 = 93.36%
0:02:30.209823s elapsed

Epoch: 6 / 100:
train: Loss: 0.0597 Accuracy: 429054 / 437991 = 97.96%
dev:   Loss: 0.2360 Accuracy: 19067 / 20454 = 93.22%
0:02:29.254880s elapsed

