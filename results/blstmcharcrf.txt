Set max num of threads to 4
Preprocess the data
Corpus(
  num of sentences: 16091
  num of words: 54303
  num of tags: 32
  num of chars: 7477
)

Load the dataset
  size of trainset: 16091
  size of devset: 803
  size of testset: 1910

Create Neural Network
  vocdim: 54303
  chrdim: 7477
  embdim: 100
  char_embdim: 200
  hiddim: 300
  outdim: 32

LSTM_CHAR(
  (embed): Embedding(54303, 100)
  (clstm): CharLSTM(
    (embed): Embedding(7477, 100)
    (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)
  )
  (wlstm): LSTM(300, 150, batch_first=True, bidirectional=True)
  (out): Linear(in_features=300, out_features=32, bias=True)
  (crf): CRF()
  (drop): Dropout(p=0.5)
  (lossfn): CrossEntropyLoss()
)

Use Adam optimizer to train the network
  epochs: 100
  batch_size: 25
  interval: 10
  eta: 0.001

Epoch: 1 / 100:
train: Loss: 5.0560 Accuracy: 409820 / 437991 = 93.57%
dev:   Loss: 5.0395 Accuracy: 18984 / 20454 = 92.81%
0:04:09.797307s elapsed

Epoch: 2 / 100:
train: Loss: 3.6504 Accuracy: 417322 / 437991 = 95.28%
dev:   Loss: 4.1359 Accuracy: 19269 / 20454 = 94.21%
0:04:21.071036s elapsed

Epoch: 3 / 100:
train: Loss: 2.9873 Accuracy: 420701 / 437991 = 96.05%
dev:   Loss: 3.8626 Accuracy: 19364 / 20454 = 94.67%
0:04:21.377852s elapsed

Epoch: 4 / 100:
train: Loss: 2.4873 Accuracy: 423690 / 437991 = 96.73%
dev:   Loss: 3.6375 Accuracy: 19432 / 20454 = 95.00%
0:04:22.122851s elapsed

Epoch: 5 / 100:
train: Loss: 2.1347 Accuracy: 425915 / 437991 = 97.24%
dev:   Loss: 3.5786 Accuracy: 19450 / 20454 = 95.09%
0:04:24.075255s elapsed

Epoch: 6 / 100:
train: Loss: 1.8938 Accuracy: 427310 / 437991 = 97.56%
dev:   Loss: 3.6078 Accuracy: 19498 / 20454 = 95.33%
0:04:25.838025s elapsed

Epoch: 7 / 100:
train: Loss: 1.6441 Accuracy: 428765 / 437991 = 97.89%
dev:   Loss: 3.5257 Accuracy: 19510 / 20454 = 95.38%
0:04:25.498782s elapsed

Epoch: 8 / 100:
train: Loss: 1.4780 Accuracy: 429672 / 437991 = 98.10%
dev:   Loss: 3.5007 Accuracy: 19505 / 20454 = 95.36%
0:04:23.058235s elapsed

Epoch: 9 / 100:
train: Loss: 1.3391 Accuracy: 430581 / 437991 = 98.31%
dev:   Loss: 3.5121 Accuracy: 19531 / 20454 = 95.49%
0:04:22.299991s elapsed

Epoch: 10 / 100:
train: Loss: 1.2153 Accuracy: 431307 / 437991 = 98.47%
dev:   Loss: 3.6190 Accuracy: 19540 / 20454 = 95.53%
0:04:23.104721s elapsed

Epoch: 11 / 100:
train: Loss: 1.1053 Accuracy: 431816 / 437991 = 98.59%
dev:   Loss: 3.6492 Accuracy: 19506 / 20454 = 95.37%
0:04:23.217776s elapsed

Epoch: 12 / 100:
train: Loss: 0.9940 Accuracy: 432506 / 437991 = 98.75%
dev:   Loss: 3.7898 Accuracy: 19503 / 20454 = 95.35%
0:04:14.314492s elapsed

Epoch: 13 / 100:
train: Loss: 0.8760 Accuracy: 433323 / 437991 = 98.93%
dev:   Loss: 3.8345 Accuracy: 19500 / 20454 = 95.34%
0:04:12.240966s elapsed

Epoch: 14 / 100:
train: Loss: 0.8078 Accuracy: 433658 / 437991 = 99.01%
dev:   Loss: 3.9389 Accuracy: 19519 / 20454 = 95.43%
0:04:12.419691s elapsed

Epoch: 15 / 100:
train: Loss: 0.7429 Accuracy: 433963 / 437991 = 99.08%
dev:   Loss: 3.9169 Accuracy: 19493 / 20454 = 95.30%
0:04:02.585858s elapsed

Epoch: 16 / 100:
train: Loss: 0.6717 Accuracy: 434419 / 437991 = 99.18%
dev:   Loss: 4.0716 Accuracy: 19491 / 20454 = 95.29%
0:04:04.232714s elapsed

Epoch: 17 / 100:
train: Loss: 0.6315 Accuracy: 434562 / 437991 = 99.22%
dev:   Loss: 4.1921 Accuracy: 19515 / 20454 = 95.41%
0:04:10.207921s elapsed

Epoch: 18 / 100:
train: Loss: 0.5680 Accuracy: 434968 / 437991 = 99.31%
dev:   Loss: 4.2460 Accuracy: 19519 / 20454 = 95.43%
0:04:05.232717s elapsed

Epoch: 19 / 100:
train: Loss: 0.4913 Accuracy: 435461 / 437991 = 99.42%
dev:   Loss: 4.2570 Accuracy: 19513 / 20454 = 95.40%
0:04:05.524071s elapsed

Epoch: 20 / 100:
train: Loss: 0.4587 Accuracy: 435635 / 437991 = 99.46%
dev:   Loss: 4.2863 Accuracy: 19497 / 20454 = 95.32%
0:04:13.331881s elapsed

Epoch: 21 / 100:
train: Loss: 0.4164 Accuracy: 435854 / 437991 = 99.51%
dev:   Loss: 4.3477 Accuracy: 19515 / 20454 = 95.41%
0:04:03.801402s elapsed

max accuracy of dev is 95.53% at epoch 10
mean time of each epoch is 0:04:15.493026s

test:  Loss: 3.4929 Accuracy: 48011 / 50319 = 95.41%
1:29:29.254650s elapsed

