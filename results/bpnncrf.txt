nohup: 忽略输入
Set max num of threads to 4
Preprocess the data
Corpus(
  num of sentences: 16091
  num of words: 383647
  num of tags: 32
  num of chars: 7477
)

Load the dataset
  size of trainset: 16091
  size of devset: 803
  size of testset: 1910

Create Neural Network
  window: 5
  vocdim: 383647
  embdim: 100
  hiddim: 300
  outdim: 32
  lossfn: cross_entropy

BPNN(
  (embed): Embedding(383647, 100)
  (hid): Linear(in_features=500, out_features=300, bias=True)
  (out): Linear(in_features=300, out_features=32, bias=True)
  (crf): CRF()
  (dropout): Dropout(p=0.5)
)

Use Adam optimizer to train the network
  epochs: 100
  batch_size: 25
  interval: 10
  eta: 0.001

Epoch: 1 / 100:
train: Loss: 7.2361 Accuracy: 401654 / 437991 = 91.70%
dev:   Loss: 6.9926 Accuracy: 18515 / 20454 = 90.52%
0:03:39.266613s elapsed

Epoch: 2 / 100:
train: Loss: 4.7234 Accuracy: 413162 / 437991 = 94.33%
dev:   Loss: 5.3992 Accuracy: 18867 / 20454 = 92.24%
0:03:36.921515s elapsed

Epoch: 3 / 100:
train: Loss: 3.4960 Accuracy: 419368 / 437991 = 95.75%
dev:   Loss: 4.9864 Accuracy: 18989 / 20454 = 92.84%
0:03:39.652952s elapsed

Epoch: 4 / 100:
train: Loss: 2.7288 Accuracy: 423472 / 437991 = 96.69%
dev:   Loss: 4.9043 Accuracy: 19044 / 20454 = 93.11%
0:03:41.011989s elapsed

